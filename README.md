### **Projeto de Machine Learning** ‚Äì Classifica√ß√£o Supervisionada no Titanic (_Survived vs Not Survived)_

![Titanic_dataset](https://raw.githubusercontent.com/Masterx-AI/Project_Titanic_Survival_Prediction_/main/titanic.jpg)

### üö¢ Contextualiza√ß√£o & Objetivos do Projeto:

> **Observa√ß√£o**: Importante ressaltar que esse projeto foi realizado como projeto pr√°tico da disciplina de Machine Learning via PUC-RS (_todos os direitos reservados_), (dataset foi adaptado e disponibilizado para cumprimento dos requistos t√©cnicos do projeto)
> no entanto, mesmo sendo uma vers√£o adaptada para os estudantes realizarem o projeto, √© um conjunto de dados provenientes do Kaggle (*https://www.kaggle.com/datasets/yasserh/titanic-dataset*)

- _<u>Dados as tarefas que s√£o mais importantes para o desenvolvimento e sucesso de uma solu√ß√£o baseada em algoritmos de Machine Learning, est√£o</u>_: a coleta e prepara√ß√£o dos dados. Na Etapa de coleta, reunimos os dados necess√°rios para a constru√ß√£o da solu√ß√£o. J√°, na etapa de prepara√ß√£o, √© analisado, filtrado e preparado os dados para aplica√ß√£o desses algoritmos.

---

- Dessa forma, os objetivos principais deste de projeto podem ser elencados em:

  - An√°lise, limpeza, filtragem & Tratamento dos Dados (EDA)
  - Sele√ß√£o das **_feautures_** que s√£o mais relevantes para a compor e dar entrada aos algoritmos de **Machine Learning**/Prepara√ß√£o dos Dados
  - Utiliza√ß√£o dos Dados preparados para constru√ß√£o de um classificador Bin√°rio de Predi√ß√£o em um problema de Classifica√ß√£o (Taxa de Predi√ß√£o entre passageiros que: Sobreviveram e N√£o sobreviveram)
  - Balanceamento do Dataset
  - Descri√ß√£o dos Algoritmos de Aprendizado Supervisionado para resolu√ß√£o do problema
  - Separa√ß√£o do subconjunto em: Treino e Teste (Estratificado) utilizando valida√ß√£o Cruzada
  - Compara√ß√£o dos resultados utilizando as medidas: Accuracy(Acur√°cia), Precision(Precis√£o), Recall (Sensibilidade) e F-Mesaure (Coeficiente F)
  - Matriz de confus√£o de cada experimento
  - Conclus√£o e hip√≥tese comentada

#### Ferramentas e Biblioteca que foram Utilizadas:

- <img 
    alt="VS Code"
    title="IDE Visual Studio Code"
    width="24px"
    style="vertical-align: middle; margin-right: 6px;"
    src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/vscode/vscode-original.svg"
  /> **Visual Studio Code**
- <img 
    alt="Python"
    title="Linguagem Python Intermedi√°ria"
    width="24px"
    style="vertical-align: middle; margin-right: 6px;"
    src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/python/python-original.svg"
  /> **Python 3.12.12**

  - Bibliotecas:

    - <img
        alt="Pandas"
        title="Biblioteca Pandas"
        width="30px"
        style="vertical-align: middle; margin-right 6px;"
        src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/pandas/pandas-original-wordmark.svg"
        />
    - <img
        alt="Numpy"
        title="Biblioteca Numpy"
        width="30px"
        style="vertical-align: middle; margin-right 6px;"
        src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/numpy/numpy-original-wordmark.svg"
        />
    - <img
        alt="ScikitLearn"
        title="Biblioteca ScikitLearn"
        width="30px"
        style="vertical-align: middle; margin-right 6px;"
        src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/scikitlearn/scikitlearn-original.svg"
        />
    - <img
        alt="Seaborn"
        title="Biblioteca Seaborn"
        width="26px"
        style="vertical-align: middle; margin-right: 6px;"
        src="https://seaborn.pydata.org/_images/logo-mark-lightbg.svg"
      /> **Seaborn**

    - <img
        alt="Matplotlib"
        title="Biblioteca Matplotlib"
        width="30px"
        style="vertical-align: middle; margin-right: 6px;"
        src="https://cdn.jsdelivr.net/gh/devicons/devicon@latest/icons/matplotlib/matplotlib-original-wordmark.svg"
      /> **Matplotlib**

    - <img
        alt="Plotly"
        title="Biblioteca Plotly Express"
        width="35px"
        style="vertical-align: middle; margin-right: 6px;"
        src="https://upload.wikimedia.org/wikipedia/commons/8/8a/Plotly-logo.png"
      /> **Plotly Express**

---

#### üé≤ **EDA**- An√°lise Explorat√≥ria de Dados

_Observa√ß√£o_: Toda as an√°lises coluna √† coluna como o tratamento para enriquecimento dos dados, tratamento de dados faltantes bem como ao seu tipo e visualiza√ß√µes e etc.. Foram comentadas passo a passo ao lonog do script do notebook do c√≥digo!

- O dataset Modificado possui as seguintes colunas:

* Tamanho/RangeIndex do dataset: 1069

**An√°lise Coluna a Coluna**
Coluna|Situa√ß√£o|Tipo de Dado
-----|-----------|--------------|
PassengerId| Identificador √önico| int64  
Survived|Alvo (0/1)|bool  
Pclass|Classe do passageiro|float64
Name|Texto livre| object
Sex|Sexo|object
SibSp|N¬∫ de irm√£os/c√¥njuges a bordo|int64  
Parch|N¬∫ de pais/filhos a bordo | int64  
Ticket|Texto livre | object
Fare|Tarifa paga|float64
Cabin|77% nulos|object
Embarked|Porto de embarque|object
day; month; year; time|Datas|int64  
cost; budget|80‚Äì90% nulos|float64
age|Idade (20% nulos)|object

> üíπ **Etapas GERAIS cumpridas durante a EDA**:

1. Visualiza√ß√£o dos dados para cada coluna
2. Formata√ß√£o de para coluna
3. Remo√ß√£o das linhas duplicadas + tratamento de dados ausentes e t√©cnicas empregadas
4. Transforma√ß√£o das colunas categ√≥ricas para num√©ricas

---

- **Subetapas por ordem de emprega√ß√£o, de acordo com o notebook**:

  - An√°lise e escolha dos atributos que ser√£o necess√°rios na entrada dos algoritmos:
  - Discuss√£o comentada em formato markdown nas c√©lulas do notebook a respeito da escolha de Atributos (Colunas) e _feature engenerring & enriquecimento dos dados_ das colunas nas quas pensei que seriam mais relevantes para entrada dos modelos e obter m√©tricas de classifica√ß√£o mais altas...

    - ETAPAS CUMPRIDAS:

      - 1. Visualiza√ß√£o dos dados
      - 2. An√°lise sobre possiveis rela√ß√£o encontradas
      - 3. Escolha das colunas mais relev√¢ntes mais justificativa comentada

- **Preenchimento de dados faltantes**:

  - ETAPAS CUMPRIDAS:

    - 1. Gerar o dataset sem as colunas com dados faltantes
    - 2. Aplicar o KNN Imputer/hot deck nas colunas + realiza√ß√µes de compara√ß√µes gr√°ficas de dispers√£o (box_plot) entre m√©todos de moda e o algoritmo empregado
    - 3. C√°lcular um atributo estat√≠stico (m√©dia, moda, mediana, etc), no dataset original, das colunas com dados faltantes
    - 4. Preencher os valores das linhas com dados faltantes com o atributo estat√≠stico (se necess√°rio) referente ao seu grupo

- **Escala dos atributos**:Agora, ser√° necess√°rio reescalar os dados, para que os algoritmos consigam aprender as rela√ß√µes entre os dados sem muito ru√≠do e melhorar a classifica√ß√£o.Obs: Para realizar essa etapa, √© necess√°rio verificar e tratar colunas com outliers. - ETAPAS CUMPRDAS - Verificar quais dados possuem outliers e tratar de acordo - Reescalar os valores

- **Checklist-Desta Etapa**:
  - Remo√ß√£o das colunas que n√£o acrescentam informa√ß√µes √∫teis;
  - Todas as colunas est√£o com a sua representa√ß√£o unificada;
  - Todas as colunas est√£o no formato num√©rico;
  - Todas as colunas est√£o com dados v√°lidos (sem dados falantes);
  - Todas as colunas est√£o reescaladas;
  - Coluna alvo balanceada.

> ü§ñ Aplica√ß√£o e Valida√ß√£o de Algoritmos de Machine Learning _Etapas Gerais Cumpridas_:

- Sele√ß√£o e treinamento de ao menos 4 algoritmos de classifica√ß√£o para para o dataset;
  -Ciclo de Treinamento, valida√ß√£o e teste do modelo dividido em: Explora√ß√£o e ajuste de hiperpar√¢metros (para os modelos escolhidos), visando o melhor resultado do classificador.
  -Valida√ß√£o dos modelos usando CROSS-VALIDATION;
- Utiliza√ß√£o da an√°lise da matriz de confus√£o e aplica√ß√£o de m√©tricas de avalia√ß√£o (**_Accuracy, Precision, Recall & F-Measure_**);
- An√°lise descritiva e comentada dos resultados, dividido em: An√°lise dos resultados de cada algoritmo e compara√ß√£o entre os algoritmos.
- Coment√°rios gerais sobre o desempenho do classificador, mencuionando acertos e discutiondo poss√≠veis raz√µes para os erros;

- _Escolha dos algoritmos,utiliza√ß√£o de t√©cnicas de modelagem (Comit√™ de Aprendizes (Ensemble Classifiers) testado no algoritmo Decision Tree), para melhorar o desempenho dos modelos e aplica√ß√£o do GridSearch explora√ß√£o dos hiper-par√¢metros_;

> Importante acrescentar, que:

Al√©m dos modelos tradicionais, utilizei tamb√©m t√©cnicas de Comit√™ de Aprendizes
(Ensemble Learning), como o BaggingClassifier. Esse tipo de abordagem combina
m√∫ltiplos aprendizes fracos,no caso a √°rvores de decis√£o, para reduzir vari√¢ncia
e aumentar a estabilidade do modelo. O objetivo do comit√™ n√£o foi validar
hiperpar√¢metros, mas sim melhorar o desempenho geral do classificador por meio da
agrega√ß√£o de v√°rios modelos independentes.

- Cria√ß√£o e testagem dos modelos que foram utilizados:
  - **_√Årvore de decis√£o_**,
  - **_MLPClassifier_**,
  - **_KNN_**,
  - **_GradienteBoostingClassifier_**, (_Gradient Boosting √© um algoritmo de Comit√™ de Aprendizes (Ensemble Learning) baseado em boosting. Ele treina v√°rios modelos fracos de forma sequencial, onde cada modelo corrige os erros do anterior. Diferente do Bagging, ele n√£o faz amostragem com reposi√ß√£o e utiliza o gradiente do erro para ajustar os modelos seguintes_),
  - **_Naive_Bayes_**,
  - **_Random Forest_** e
  - **_Dummy_** para baseline

#### Modelos Selecionados para Hiperparametriza√ß√£o

Ap√≥s a etapa inicial de testes, alguns modelos foram descartados por apresentarem
desempenho inferior ou por n√£o se adequarem bem ao problema. Assim, para a etapa de
busca pelos melhores hiperpar√¢metros (GridSearchCV), foram selecionados os seguintes
modelos:

- **Decision Tree Classifier**
- **Random Forest Classifier**
- **MLPClassifier (Rede Neural Artificial)**
- **K-Nearest Neighbors (KNN)**
- **GradientBoostingClassifier**

Al√©m desses, foi inclu√≠do tamb√©m o:

- **DummyClassifier (Baseline)**  
  _O DummyClassifier √© utilizado como refer√™ncia m√≠nima de desempenho. Ele n√£o aprende
  padr√µes dos dados; apenas gera previs√µes constantes ou aleat√≥rias. Por esse motivo,
  n√£o possui hiperpar√¢metros relevantes para ajuste via GridSearch. Seu papel √©
  demonstrar o desempenho m√≠nimo esperado, permitindo avaliar se os modelos reais est√£o
  de fato aprendendo e superando o baseline._

#### Execu√ß√£o do Treinamento e Valida√ß√£o usando Cross-Validation (_Conjunto de dados pequeno 1069 entradas, ap√≥s toda etapa de EDA, reduzido a 890 entradas_):

- Mesmo que o GridSearchCV use internamete a valida√ß√£o cruzada (por exemplo, cv=5) para escolher os melhores hiperpar√¢metrosm, ainda n√£o foi feito uma valida√ß√£o cruzada final comparando todos os modelos com as m√©tricas necess√°rias:

  metrics = {
  'accuracy': 'Accuracy',
  'f1': 'F1-Score',
  'roc_auc': 'ROC-AUC',
  'precision': 'Precision',
  'recall': 'Recall'
  }

**_DESEMPENHO DOS MODELOS (KFold - 10 splits)_**:
| Modelo | Accuracy | F1-Score | ROC-AUC | Precision | Recall | Tempo_Treino (s) |
|-------------------|--------------------|--------------------|--------------------|--------------------|--------------------|------------------|
| Gradient Boosting | 0.8371 ¬± 0.0435 | 0.7829 ¬± 0.0539 | 0.8864 ¬± 0.0403 | 0.7974 ¬± 0.0798 | 0.7784 ¬± 0.0808 | 0.39 |
| Random Forest | 0.8286 ¬± 0.0423 | 0.7672 ¬± 0.0501 | 0.8934 ¬± 0.0421 | 0.8008 ¬± 0.0714 | 0.7481 ¬± 0.0939 | 0.36 |
| Decision Tree | 0.8175 ¬± 0.0546 | 0.7499 ¬± 0.0677 | 0.8743 ¬± 0.0490 | 0.7976 ¬± 0.1094 | 0.7259 ¬± 0.1113 | 0.00 |
| MLP | 0.8216 ¬± 0.0348 | 0.7486 ¬± 0.0448 | 0.8718 ¬± 0.0368 | 0.8074 ¬± 0.0823 | 0.7063 ¬± 0.0707 | 1.81 |
| KNN | 0.7964 ¬± 0.0581 | 0.7316 ¬± 0.0638 | 0.8782 ¬± 0.0510 | 0.7458 ¬± 0.0933 | 0.7303 ¬± 0.0925 | 0.00 |
| Dummy | 0.5435 ¬± 0.0586 | 0.3442 ¬± 0.0969 | 0.5012 ¬± 0.0645 | 0.3874 ¬± 0.1350 | 0.3165 ¬± 0.0821 | 0.04 |

### > üéâ Conclus√£o:

#### > De acordo com a execu√ß√£o dos meus modelos, ap√≥s valida√ß√£o cruzada usando o K-fold (por seu um conjunto de dados pequeno);

> Ap√≥s a execu√ß√£o dos modelos e a valida√ß√£o cruzada utilizando **K-Fold com 10 divis√µes**, foi poss√≠vel comparar o desempenho m√©dio de cada algoritmo com base nas principais m√©tricas de classifica√ß√£o. Os resultados obtidos para esse _famoso_ dataset de treinamento de Machine Learning do Kaggle, demonstram diferen√ßas (dado a forma que executei e as decis√µes que escolhi tomar), serem relevantes entre os modelos, tanto em desempenho quanto em estabilidade.

#### **_1._** **Melhor desempenho Geral: Gradiente Boosting**

O Gradient Boosting apresentou o melhor equil√≠brio entre as m√©tricas avaliadas, alcan√ßando:

- **Acur√°cia**: 83.71%
- **F1-Score**: 78.29%
- **Recall** (_Sensibilidade_): 77.84%
- **ROC-AUC**: 88.64%

Tais valores expressos pelos m√©tricas de c√°lculo, idnicam que o modelo conseguiu identificar corretamente tanto sobreviventes quanto n√£o sobreviventres, mantendo boa capacidade de genereliza√ß√£o (na medida do poss√≠vel). Entre todos os algoritmos testatos, foi o mais consistente e robusto.

#### **_2_**. **Segundo melhor modelo: Random Forest**

O Random Forest apresentou desempenho muito pr√≥ximo ao Gradient Boosting, com:

- **Acur√°cia**: 82.86%
- **F1-Score**: 76.72%
- **ROC-AUC**: 89.34% (melhor entre todos os modelos)
- **Precis√£o**: 80.08%

Apesar de ligeiramente inferior em F1-Score e Recall, o Random Forest superou o Gradiente Boosting (dado todas as condi√ß√µes de testagem que realizei) em **Precis√£o** e **ROC-AUC**, mostrando excelente capacidade de discrimina√ß√£o entre as classes

#### **_3_** **Modelos Intermedi√°rios**

Os modelos **_Decision Tree_**, **_MLPClassifier_** e **_KNN_** apresentaram resultados aceit√°veis:

- **Decision Tree**: ~81.75% de acur√°cia
- **MLPClassifier**: ~82.16%
- **KNN**: ~79.64%
  Embora inferiores aos ensembles, ainda assim, superaram amplamente o baseline, demonstrando que extra√≠ram padr√µes relevantes do dataset.

#### **_4. Baseline_**: **Dummy Classifier**

O DummyClassifier obteve apenas:

- **Acur√°cia**: 54.35%
- **F1-Score**: 34.42%
  Esse desempenho confirma que os modelos supervisionados realmente aprenderam padr√µes significativos, j√° que todos superaram o baseline com ampla margem

#### **5. M√©tricas mais relevantes para o problema**

Como o OBJETIVO √© prever quem sobreviveu ao naufr√°gio, as m√©tricas mais importantes s√£o:

- **Recall (Sensibilidade)**: minimizar falsos negativos (n√£o prever um sobrevivente)

- **ROC-AUC**: avaliar a capacidade de separa√ß√£o entre as classes
  Errar um sobrevivente √© mais cr√≠tico do que errar um n√£o sobrevivente, o que torna o Recall uma m√©trica essencial. O Gradient Boosting apresentou o melhor equil√≠brio entre Recall e F1-Score, enquanto o Random Forest se destacou no ROC-AUC.

#### IMPORTANTE: üôãüèª‚Äç‚ôÇÔ∏è **6. Considera√ß√µes sobre custo computacional**

O Gradient Boosting apresentou o maior tempo de execu√ß√£o durante a busca pelos melhores hiperpar√¢metros, levando aproximadamente 5 minutos e 41 segundos em m√°quina local. Apesar disso, o ganho de desempenho justifica o custo computacional em cen√°rios onde a precis√£o √© priorit√°ria.

#### Plotagem geradas:

## üß© **Limita√ß√µes do Estudo**

√â importante ressalta que: este projeto representa minha primeira experi√™ncia pr√°tica com algoritmos de Aprendizado de M√°quina envolvendo t√©cnicas de Comit√™s de Aprendizes (Ensemble Learning),
como Bagging, e Gradient Boosting. Embora os resultados tenham sido satisfat√≥rios, algumas limita√ß√µes devem ser consideradas:

1. **Tamanho reduzido do dataset**  
   O conjunto de dados do Titanic √© relativamente pequeno, o que limita a capacidade
   dos modelos de capturar padr√µes mais complexos e aumenta a variabilidade entre os folds.

2. **Depend√™ncia de pr√©-processamento manual**  
   Algumas decis√µes de limpeza, transforma√ß√£o e engenharia de atributos foram feitas
   manualmente. Outras abordagens poderiam gerar features mais informativas.

3. **Explora√ß√£o limitada de hiperpar√¢metros**  
   Apesar do uso de GridSearchCV, a busca foi restrita a um conjunto espec√≠fico de
   hiperpar√¢metros devido ao custo computacional em m√°quina local.

4. **Pouca experimenta√ß√£o com outros ensembles avan√ßados**

   Modelos como XGBoost, LightGBM e CatBoost n√£o foram explorados, embora sejam
   refer√™ncias modernas em boosting.

5. **Custo computacional**  
   O Gradient Boosting, por exemplo, levou mais de 5 minutos para encontrar a melhor
   combina√ß√£o de hiperpar√¢metros, o que limita experimentos mais amplos.

## üöÄ Trabalhos Futuros

Com base nas limita√ß√µes identificadas, algumas melhorias e extens√µes podem ser
implementadas em vers√µes futuras deste projeto:

1. **Explorar ensembles mais modernos**  
   Incluir algoritmos como XGBoost, LightGBM e CatBoost, que oferecem melhor
   desempenho e menor tempo de treinamento.

2. **Aprimorar a engenharia de atributos**  
   Criar novas vari√°veis derivadas (ex.: tamanho da fam√≠lia, t√≠tulo social,
   agrupamento de idades) para enriquecer o poder preditivo dos modelos.

3. **Aplicar t√©cnicas de balanceamento**  
   M√©todos como SMOTE ou class weights podem melhorar o Recall para a classe
   minorit√°ria (sobreviventes).

4. **Automatizar o pipeline**  
   Utilizar ferramentas como `Pipeline` e `ColumnTransformer` para padronizar
   pr√©-processamento e reduzir risco de vazamento de dados.

5. **Avaliar interpretabilidade**  
   Aplicar SHAP ou LIME para entender melhor a contribui√ß√£o de cada feature nos
   modelos ensemble.

6. **Comparar desempenho em ambientes mais robustos**  
   Executar a hiperparametriza√ß√£o em m√°quinas mais potentes ou ambientes em nuvem
   para expandir a busca de par√¢metros.

### Autor:

**Lucas Lelis**

- _Projeto Pr√°tico ***ADAPTADO*** da Disciplina de Machine Learning-PUC-RS_
- _todos os direitos reservados_
